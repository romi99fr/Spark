{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ee0ce3",
   "metadata": {},
   "source": [
    "# Create the Spark session\n",
    "This is required to create the first dataframe and start working with Spark.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we need to install the package\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Path to the resources folder of the project, use the full path on Windows\n",
    "HADOOP_HOME = r\"./hadoop_home\"\n",
    "#Use the full path on Windows for PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON, for instance: r\"C:\\SOFT\\Python3\\python.exe\"\n",
    "PYSPARK_PYTHON = r\"python.exe\"\n",
    "PYSPARK_DRIVER_PYTHON = r\"python.exe\"\n",
    "#Need to provide the path to the PostgreSQL driver to create a connection later on\n",
    "POSTGRESQL_DRIVER_PATH = r\"<DIRECTORY>/postgresql-42.3.6.jar\"\n",
    "\n",
    "if(__name__== \"__main__\"):\n",
    "    os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
    "    sys.path.append(HADOOP_HOME + \"\\\\bin\")\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_DRIVER_PYTHON\n",
    "\n",
    "    # Create the configuration in the local machine and give a name to the application (which will appear in the GUI)\n",
    "    conf = SparkConf() \\\n",
    "        .set(\"spark.master\", \"local\") \\\n",
    "        .set(\"spark.app.name\", \"Spark Dataframes Tutorial\") \\\n",
    "        .set(\"spark.jars\", POSTGRESQL_DRIVER_PATH)\n",
    "\n",
    "    # Create the session \n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    print(f\"Python version = {spark.sparkContext.pythonVer}\")\n",
    "    print(f\"Spark version = {spark.version}\")\n",
    "    print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e984ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the level of parallelism configured (equal to the number of cores is obtained with \"*\")\n",
    "print(\"Master: \",spark.conf.get(\"spark.master\"))\n",
    "print(\"Parallelism: \",spark.sparkContext.defaultParallelism)\n",
    "print(\"Minimum number of partitions: \",spark.sparkContext.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe473f-1341-4e6e-b883-3d836962e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the session and create a new one as above, to change the degree of parallelism\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb6ab",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Create a Dataframe\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9989-e6ee-4db3-a282-2785db1303c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an unnamed data structure\n",
    "data = (('a', 1, 1.1,True),\n",
    "        ('b', 2, 2.2, False),\n",
    "        ('c', 3, None, True),\n",
    "        ('d', 4, 4.4, None),\n",
    "        ('e', 5, 5.5, True)\n",
    "       )\n",
    "# Create a dataframe from the data\n",
    "spark.createDataFrame(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f0d21-701c-45c1-80f3-259c6f40b388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an unnamed data structure\n",
    "data = (('a', 1, 1.1,True),\n",
    "        ('b', 2, 2.2, False),\n",
    "        ('c', 3, None, True),\n",
    "        ('d', 4, 4.4, None),\n",
    "        ('e', 5, 5.5, True)\n",
    "       )\n",
    "# Create a dataframe from the data, but given attribute names\n",
    "spark.createDataFrame(data, schema=(\"A\",\"B\",\"C\",\"D\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56d94c-6a15-4e81-94d7-36afca2a7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data structure\n",
    "data = ({\"A\": 'a', \"B\": 1, \"C\": 1.1, \"D\": True},\n",
    "        {\"A\": 'b', \"B\": 2, \"C\": 2.2, \"D\": False},\n",
    "        {\"A\": 'c', \"B\": 3, \"D\": True},\n",
    "        {\"A\": 'd', \"B\": 4, \"C\": 4.4},\n",
    "        {\"A\": 'e', \"B\": 5, \"C\": 5.5, \"D\": True}\n",
    "       )\n",
    "# Create a dataframe from the data\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db3ef3-10a9-42cc-813c-968df0dd61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Show the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b746c-6ea1-489e-9128-9e3eeb078b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Describe the Dataframe\n",
    "df.summary().show() # Also with \"df.describe().show()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a7ecf-9804-4924-962b-8026b93f050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the dataframe in memory once built\n",
    "df=df.cache()\n",
    "print(\"The dataframe is kept in memory now!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d13b3a-d28f-4332-82b2-bfaa38f5160d",
   "metadata": {},
   "source": [
    "## A Spark Dataframe is differenf from Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Spark Dataframe into a Pandas Dataframe\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862325b6-c0cf-4554-a1e1-893109412ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the columns\n",
    "print(\"Columns: \",df.columns)\n",
    "# Print the schema\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa032c61-e1dd-4815-8947-6bd87c047467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refering to the column, we get a Column object\n",
    "# We can do it by name or by position\n",
    "df[\"A\"] # Equivalent to \"df[0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be3b5c-6773-45c8-b3f3-80120f3d14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and columns cannot be directly referred like in Pandas\n",
    "print(\"Column: \\n\",df.toPandas()[\"A\"])\n",
    "print(\"Row: \\n\",df.toPandas().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d14f0-bda2-4aa5-8967-e641e49d873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of partitions (it should coincide with the parallelism configured)\n",
    "print(\"Number of partitions: \",df.rdd.getNumPartitions())\n",
    "# Partitions\n",
    "print(\"Partitions: \",df.rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde92e30",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64686072",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts how many rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be3a36",
   "metadata": {},
   "source": [
    "## first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the first row\n",
    "# A Row is simply a tuple\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad26553",
   "metadata": {},
   "source": [
    "## collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91dcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Retrieves all the rows in the dataframe\n",
    "# Simply a list of tuples\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26b6ca",
   "metadata": {},
   "source": [
    "## take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83678736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves n rows\n",
    "print(\"Take\",df.take(3))\n",
    "print(\"Head\",df.head(3))\n",
    "print(\"Tail\",df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78392f0",
   "metadata": {},
   "source": [
    "## write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94891b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file with header\n",
    "df.write.csv(path=\"output/CSVfile\", mode=\"overwrite\", header=True)\n",
    "print(\"CSV written!!!\")\n",
    "# Create a JSON file\n",
    "df.write.json(path=\"output/JSONfile\", mode=\"overwrite\")\n",
    "print(\"JSON written!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c2953-a61b-40c0-befd-ea7d824089cb",
   "metadata": {},
   "source": [
    "### Writting/Reading a partitioned the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994a6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repartition the dataframe\n",
    "dfRepartitioned=df.repartition(5)\n",
    "# Print the number of partitions\n",
    "print(\"Number of partitions: \",dfRepartitioned.rdd.getNumPartitions())\n",
    "# Partitions\n",
    "print(\"Partitions: \",dfRepartitioned.rdd.glom().collect())\n",
    "# Create a CSV file\n",
    "dfRepartitioned.write.csv(path=\"output/CSVfilePartitioned\", header='True', mode=\"overwrite\")\n",
    "print(\"CSV written!!!\")\n",
    "# Create a JSON file\n",
    "dfRepartitioned.write.json(path=\"output/JSONfilePartitioned\", mode=\"overwrite\")\n",
    "print(\"JSON written!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRecovered = spark.read.csv(path=\"output/CSVfilePartitioned\", header='True', inferSchema='True')\n",
    "# Print the number of partitions (it depends on the number of files in the directory and the degree of parallelism of the session)\n",
    "print(\"Number of partitions: \",dfRecovered.rdd.getNumPartitions())\n",
    "# Partitions\n",
    "print(\"Partitions: \",dfRecovered.rdd.glom().collect())\n",
    "# Show the data\n",
    "dfRecovered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f67646",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data into a Dataframe\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5f291-2a81-49d7-b9ec-a1785a9c5ee0",
   "metadata": {},
   "source": [
    "## From files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b4cb4",
   "metadata": {},
   "source": [
    "### Automatically infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set File Path\n",
    "airportsFilePath = \"flight-data/airport-codes-na.txt\" # From http://openflights.org/data.html\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t').cache()\n",
    "print(\"File loaded!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe01505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of the dataframe\n",
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the contents of the dataframe\n",
    "airports.show(5) # By default shows the top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de450636",
   "metadata": {},
   "source": [
    "### Manually provide the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7202abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set File Path\n",
    "flightPerfFilePath = \"flight-data/departuredelays.csv\" # From https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true', inferSchema='false').cache()\n",
    "print(\"File loaded!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the inferred schema\n",
    "flightPerf.printSchema()\n",
    "print(\"... which actually corresponds to: \",flightPerf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8260e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first row\n",
    "flightPerf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the basic statistics of each column\n",
    "flightPerf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f6ce0",
   "metadata": {},
   "source": [
    "#### Create a more accurate schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# The schema is encoded using StructType and using various pyspark.sql.types\n",
    "newSchema = StructType([\n",
    "    StructField(\"date\", StringType(), False),    \n",
    "    StructField(\"delay\", IntegerType(), False),\n",
    "    StructField(\"distance\", IntegerType(), False),\n",
    "    StructField(\"origin\", StringType(), False),\n",
    "    StructField(\"destination\", StringType(), False)\n",
    "])\n",
    "print(\"New schema created!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297842ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes are immutable, so we have to define a new one\n",
    "flightPerf.schema = newSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1eff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to cast the two integer attributes\n",
    "casted = flightPerf \\\n",
    "    .withColumn(\"delay\", flightPerf[\"delay\"].cast(\"Integer\")) \\\n",
    "    .withColumn(\"distance\", flightPerf[\"distance\"].cast(\"Integer\")) \n",
    "casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Avoid using collect, use RDD instead\n",
    "flightPerfWithSchema = spark.createDataFrame(data=casted.collect(), schema=newSchema, verifySchema=True) \n",
    "flightPerfWithSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Avoid using collect, use RDD instead\n",
    "flightPerfWithSchema = spark.createDataFrame(data=casted.rdd, schema=newSchema, verifySchema=True)\n",
    "flightPerfWithSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d948f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without schema\")\n",
    "flightPerf.describe().show()\n",
    "print(\"With schema\")\n",
    "flightPerfWithSchema = flightPerfWithSchema.cache()\n",
    "flightPerfWithSchema.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ec6ff-0e02-42cd-b635-d992dfa31569",
   "metadata": {},
   "source": [
    "## From a Database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5f8bc-683b-496a-afa5-0b56ecd87a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create a Spark Dataframe from a pre-existing table through a JDBC connection\n",
    "JDBCdf = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/<DATABASENAME>\") \\\n",
    "    .option(\"dbtable\", \"<SCHEMA>.<TABLE>\") \\\n",
    "    .option(\"user\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .load()\n",
    "# This is equivalent to\n",
    "# JDBCdf = spark.read.jdbc(url=\"jdbc:postgresql://localhost:5432/<DATABASENAME>?user=\"<USERNAME>\"&password=\"<PASSWORD>\",table=\"<SCHEMA>.<TABLE>\",properties={\"driver\": \"org.postgresql.Driver\"})\n",
    "JDBCdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e95681",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97a10d-0544-4535-973d-41deb4da0543",
   "metadata": {
    "tags": []
   },
   "source": [
    "## withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1dc7f-696e-4422-805c-35fa1bf68818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can derive new columns from existing ones\n",
    "df.withColumn(\"derived\",df[2]*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf823315-9b83-4d07-b506-302daf7301c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# We can derive new columns\n",
    "#df.withColumn(\"constant\",666).show() # This does not work\n",
    "df.withColumn(\"derived\",df[\"B\"]*2).withColumn(\"constant\",lit(666)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ad75c-2490-48e1-a12f-e0ca72cdaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# Add a new column with a surrogate\n",
    "dfWithKey = df.withColumn(\"Key\",monotonically_increasing_id())\n",
    "dfWithKey.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad961f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc962f7-f503-4227-a4ca-de3648ce1e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#There are multiple ways to refer a column\n",
    "df.select(\"A\",\"B\").show()\n",
    "columns=(\"A\",\"B\")\n",
    "df.select(*columns).show()\n",
    "df.select(df.A,df.B).show()\n",
    "df.select(df[\"A\"],df[\"B\"]).show()\n",
    "df.select(col(\"A\"),col(\"B\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1047c9b-67fc-4ade-a3b1-1f74d7a4c2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can select all the columns with the typical wild card\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bf64b-e9db-44fb-bf5a-643c7dc67572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "# We can merge different columns in a single one\n",
    "# In this case, it must be a struct, because they are heterogenous\n",
    "# If the columns are homogeneous, they can form an array or map\n",
    "dfKeyValue = dfWithKey.select(\"Key\",struct(\"A\", \"B\", \"C\", \"D\").alias(\"Value\"))\n",
    "dfKeyValue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4e9a0-7291-411e-90f6-76ed4f1c888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import explode\n",
    "\n",
    "# We can extract the elements of the struct one by one\n",
    "# If it were an array or map, we could simply use \"explode\" to extract all at once\n",
    "dfKeyValue.select((col(\"Value\"))[\"A\"].alias(\"A\"), (col(\"Value\"))[\"B\"].alias(\"B\"), (col(\"Value\"))[\"C\"].alias(\"C\"), (col(\"Value\"))[\"D\"].alias(\"D\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafe9f2-0f46-48d7-8775-4489cb79f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# We can merge different columns in an ML vector\n",
    "# As soon as they are numerical and without NULL\n",
    "VectorAssembler(inputCols=[\"B\", \"C\"], outputCol=\"features\").transform(df.select(\"B\",\"C\").filter(\"C IS NOT NULL\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3dbd8",
   "metadata": {},
   "source": [
    "## filter/where\n",
    "\n",
    "This way, we can refer to any row,  without the need of an explicit \"index\" column (i.e., we can use any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5c965-b033-48b2-ae0e-6145b48fa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"A='a'\").show()\n",
    "df.filter(\"B=1\").show()\n",
    "df.where(\"D AND C IS NULL\").show()\n",
    "df.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d6ccc",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9da8f-7bbf-4ee3-8a1b-e919d6870702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(withReplacement=False, fraction=0.4, seed=666).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42f6e5",
   "metadata": {},
   "source": [
    "## distinct/dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832803a-5b1d-4dd1-94c1-a24f9aeb0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes duplicate rows\n",
    "df.select(\"D\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a10ec3-7c0e-48aa-9b05-9bdcd60b9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes rows that have duplicate values in the given columns\n",
    "df.dropDuplicates([\"D\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1f671-343a-48db-a1b0-3c68a7f8fc69",
   "metadata": {},
   "source": [
    "## sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddbc74-4a6b-4eb7-a945-6c1518c3a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "df.sort(\"A\").show()\n",
    "df.sort(asc(\"D\"), desc(\"B\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d7126",
   "metadata": {},
   "source": [
    "## replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbd71f-36d4-4b94-9f0d-5be6425f78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can replace any value in any column, all at once\n",
    "df.replace(to_replace=[3,4,5], value=[33,44,None], subset=[\"B\"]).show()\n",
    "print(\"Notice the original dataframe has not really changed!!!\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab331670-3c83-4fe9-a3f6-34d82e7d40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work for NULL df.replace(to_replace=[None], value=[3.3], subset=[\"C\"]).show()\n",
    "df.fillna(3.3,\"C\").show()\n",
    "# We can indicate a map for different imputations per column\n",
    "df.fillna({\"C\": 3.3, \"D\": False}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c376092",
   "metadata": {
    "tags": []
   },
   "source": [
    "## groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38b507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can obtain an object that allows to apply aggregation functions to it\n",
    "# You cannot show or print this object (it is not a Dataframe)\n",
    "print(df.count())\n",
    "df.groupBy(\"D\").count().show()\n",
    "df.groupBy(\"D\").max(\"B\",\"C\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0724d2",
   "metadata": {},
   "source": [
    "## agg\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54f300-3513-4617-9c41-9cb5fca0ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df.groupBy(\"D\").agg(fn.count(\"*\")).show()\n",
    "df.groupBy(\"D\").agg(fn.count(\"D\")).show() #Giving the column name, counts those not null\n",
    "df.groupBy(\"D\").agg(fn.max(\"B\"), fn.max(\"C\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db679e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a shorthand for df.groupBy().agg()\n",
    "df.agg(\n",
    "    fn.count('*').alias('count')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330ef96",
   "metadata": {},
   "source": [
    "## crossJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a2f1e-d3db-41a0-aa5d-2ad18291f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"A\").crossJoin(df.select(\"B\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e5f59",
   "metadata": {},
   "source": [
    "## join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d76aa-4ec7-43d0-966b-fe18540342d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendf = df.withColumnRenamed(\"A\",\"RA\") \\\n",
    "    .withColumnRenamed(\"B\",\"RB\") \\\n",
    "    .withColumnRenamed(\"C\",\"RC\") \\\n",
    "    .withColumnRenamed(\"D\",\"RD\")\n",
    "df.join(rendf, [df.A==rendf.RA, df.C==rendf.RC]).show() # What happened with row number 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60106e07-2bf0-4a5c-9cc2-fac734c282ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take bigger Dataframes\n",
    "print(\"Airports(Spark): \", airports.count())\n",
    "print(\"Flights(Spark): \", flightPerfWithSchema.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798d0dc-ffff-4ab4-8b47-56551856d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Join the two big Dataframes\n",
    "flightPerfWithSchema.join(airports, [flightPerfWithSchema.origin==airports.IATA]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803e3ba-289c-49a1-a11a-e86016f55911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's do the join as an operation derived from cross product and filter\n",
    "crossed = flightPerfWithSchema.crossJoin(airports)\n",
    "crossed.where(crossed.origin==crossed.IATA).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236ab89-efff-4f9c-a3cd-c28b1b2aee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Spark Dataframes into Pandas\n",
    "p_airports = airports.toPandas()\n",
    "p_flightPerfWithSchema = flightPerfWithSchema.toPandas()\n",
    "print(\"Airports(Pandas): \", len(p_airports))\n",
    "print(\"Flights(Pandas): \", len(p_flightPerfWithSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b519eef-6d19-49c3-aec4-fd0c9f395651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's take again the join as an operation derived from cross product and filter on Pandas Dataframes\n",
    "# THIS CAN TAKE A WHILE!!!\n",
    "crossed = p_flightPerfWithSchema.join(p_airports, how=\"cross\")\n",
    "len(crossed[crossed[\"origin\"]==crossed[\"IATA\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd768f7-d079-4acd-8266-18e6d67ce1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's do only the cross product on the Spark Dataframe\n",
    "crossed = flightPerfWithSchema.crossJoin(airports)\n",
    "crossed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37618189",
   "metadata": {
    "tags": []
   },
   "source": [
    "## repartition/coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad97e5c-dda9-407d-9b04-5cd641b53c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Number of Partitions: \", flightPerfWithSchema.rdd.getNumPartitions())\n",
    "rep = flightPerfWithSchema.repartition(1)\n",
    "print(\"Number of Partitions: \", rep.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee677e9-67df-4977-97aa-fc8ed4165c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Number of Partitions: \", flightPerfWithSchema.rdd.getNumPartitions())\n",
    "coa = flightPerfWithSchema.coalesce(1)\n",
    "print(\"Number of Partitions: \", coa.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fe63f-f530-4a19-b045-66e0efb7d616",
   "metadata": {},
   "source": [
    "## Transformations on RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e434c2",
   "metadata": {},
   "source": [
    "### map\n",
    "For each element in the RDD returns exactly another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e3069-484e-407c-9f41-eedc1cad98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.map(lambda r: r[\"A\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d2fb1-21c6-4ba5-94b8-b53698a70206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd \\\n",
    "    .map(lambda r: (r[\"A\"],r[\"B\"]-1,r[\"B\"]*2,r[\"D\"]) if r[3] else (None,\"X\",\"Y\",\"Z\")) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d61c89-db54-4945-aeff-518c8cc0cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd \\\n",
    "    .map(lambda r: (r[\"A\"],r[\"B\"]-1,r[\"B\"]*2,r[\"D\"]) if r[3] else None) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70728ff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### flatmap\n",
    "Returns a (potentially empty) list of elements of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1b08c-37d5-4ba8-b27a-b7d24e9335a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd \\\n",
    "    .flatMap(lambda r: [r[\"A\"],r[\"A\"]] if r[\"D\"] else ()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f731622-55f1-4bd7-828f-91ec3a958c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd \\\n",
    "    .flatMap(lambda r: [(r[\"A\"],r[\"B\"]-1), (r[\"B\"]*2,False)] if r[3] else ()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982825c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### reduce\n",
    "Actually, this is an action (not a transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b4cd-db7f-411e-b2c7-c7b2084fe33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd \\\n",
    "    .map(lambda r: r[\"B\"]) \\\n",
    "    .reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4a37d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/catalog.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484f327",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Link the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the characteristics of the default database\n",
    "print(spark.catalog.listDatabases()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the characteristics of the database\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED default;\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ea752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table in the spark session default database for each one of the dataframes\n",
    "# This allows to access them as any other Relational table using SQL\n",
    "airports.createOrReplaceTempView(\"Airports\")\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "print(\"Temporary views created!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing table names \n",
    "for t in spark.catalog.listTables():\n",
    "    print(t.tableType,\" \",t.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf71be2-6702-4183-a437-944da4cf0e93",
   "metadata": {},
   "source": [
    "## Execute queries using standard SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007109f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "SELECT City\n",
    "FROM Airports\n",
    "LIMIT 5;\n",
    "\"\"\")\n",
    "result.printSchema()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.City, f.origin, sum(f.delay) as Delays \n",
    "FROM FlightPerformance f \n",
    "    JOIN airports a \n",
    "      ON a.IATA = f.origin \n",
    "WHERE a.State = 'WA' \n",
    "GROUP BY a.City, f.origin \n",
    "ORDER BY Delays desc;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f64909-def9-4b71-946c-3cb02491abcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This does not work, only queries are allowed\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO airports(City, State, Country, IATA) VALUES ('A','B','C','D');\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
